{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Kubeflow Pipeline!\n",
    "\n",
    " - Step 1: Create a custom component for creating training/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./pipeline\n",
    "!mkdir ./pipeline/components\n",
    "!mkdir ./pipeline/components/bq2parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The steps of creating a custom component are \n",
    "- Create the task code\n",
    "- Create the Dockerfile\n",
    "- Build and push Image\n",
    "- Create the config.yaml\n",
    "\n",
    "#### Step 1: Write the task code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/components/bq2parquet/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/components/bq2parquet/main.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery.job import ExtractJobConfig\n",
    "\n",
    "\n",
    "DATASET = \"credit_card_fraud\"\n",
    "TRAIN_TABLE = \"train\"\n",
    "VALID_TABLE = \"validation\"\n",
    "\n",
    "TRAIN_SQL = \"\"\" CREATE OR REPLACE TABLE credit_card_fraud.train AS\n",
    "WITH features_table AS (\n",
    "    SELECT IFNULL(TransactionDT, 0) AS TransactionDT, \n",
    "    IFNULL(TransactionAmt, 0.0) AS TransactionAmt, \n",
    "    IFNULL(card1,0) AS card1, IFNULL(card2,0.0) AS card2, \n",
    "    IFNULL(card3,0.0) AS card3, IFNULL(C1,0.0) AS C1, IFNULL(C2,0.0) AS C2, \n",
    "    IFNULL(C11,0.0) AS C11, IFNULL(C12,0.0) AS C12, IFNULL(C13,0.0) AS C13, \n",
    "    IFNULL(C14,0.0) AS C14, IFNULL(D8,0.0) AS D8, IFNULL(V45,0.0) AS V45, \n",
    "    IFNULL(V87,0.0) AS V87, IFNULL(V258,0.0) AS V258, \n",
    "    IFNULL(card6, \"Unknown\") AS card6, IFNULL(ProductCD, \"Unknown\") AS ProductCD,\n",
    "    IFNULL(P_emaildomain, \"Unknown\") AS emaildomain,isFraud \n",
    "    FROM `kubeflow-1-0-2.credit_card_fraud.data`\n",
    "    WHERE isFraud IS NOT NULL)\n",
    "\n",
    "SELECT * FROM features_table AS data\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(data))), 10) < 8\n",
    "\"\"\"\n",
    "\n",
    "VALID_SQL = \"\"\" CREATE OR REPLACE TABLE credit_card_fraud.validation AS\n",
    "WITH features_table AS (\n",
    "    SELECT IFNULL(TransactionDT, 0) AS TransactionDT, \n",
    "    IFNULL(TransactionAmt, 0.0) AS TransactionAmt, \n",
    "    IFNULL(card1,0) AS card1, IFNULL(card2,0.0) AS card2, \n",
    "    IFNULL(card3,0.0) AS card3, IFNULL(C1,0.0) AS C1, IFNULL(C2,0.0) AS C2, \n",
    "    IFNULL(C11,0.0) AS C11, IFNULL(C12,0.0) AS C12, IFNULL(C13,0.0) AS C13, \n",
    "    IFNULL(C14,0.0) AS C14, IFNULL(D8,0.0) AS D8, IFNULL(V45,0.0) AS V45, \n",
    "    IFNULL(V87,0.0) AS V87, IFNULL(V258,0.0) AS V258, \n",
    "    IFNULL(card6, \"Unknown\") AS card6, IFNULL(ProductCD, \"Unknown\") AS ProductCD,\n",
    "    IFNULL(P_emaildomain, \"Unknown\") AS emaildomain,isFraud \n",
    "    FROM `kubeflow-1-0-2.credit_card_fraud.data`\n",
    "    WHERE isFraud IS NOT NULL)\n",
    "\n",
    "SELECT * FROM features_table AS data\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(data))), 10) >= 8\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def export_table_to_gcs(dataset_ref, \n",
    "                        source_table, \n",
    "                        destination_uri):\n",
    "    table_ref = dataset_ref.table(source_table)\n",
    "\n",
    "    config = ExtractJobConfig()\n",
    "    config.print_header = False\n",
    "    config.destination_format=\"PARQUET\"\n",
    "\n",
    "    extract_job = bq.extract_table(\n",
    "                                    table_ref,\n",
    "                                    destination_uri,\n",
    "                                    job_config=config,\n",
    "                                  )\n",
    "    extract_job.result()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "                        \"--export_path\",\n",
    "                        help = \"Path to export the train.parquet and eval.parquet files\",\n",
    "                        required = True\n",
    "                        )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_path = args.export_path\n",
    "    \n",
    "    train_export_path = os.path.join(data_path, \n",
    "                                     \"train.parquet\")\n",
    "    valid_export_path = os.path.join(data_path, \n",
    "                                     \"eval.parquet\")\n",
    "\n",
    "    bq = bigquery.Client()\n",
    "\n",
    "    dataset_ref = bigquery.Dataset(bq.dataset(\"credit_card_fraud\"))\n",
    "\n",
    "    try:\n",
    "        bq.create_dataset(dataset_ref)\n",
    "        print(\"Dataset created\")\n",
    "    except:\n",
    "        print(\"Dataset already exists\")\n",
    "\n",
    "    print(\"Creating the training dataset...\")\n",
    "    bq.query(TRAIN_SQL).result()\n",
    "\n",
    "    print(\"Creating the validation dataset...\")\n",
    "    bq.query(VALID_SQL).result()\n",
    "\n",
    "    print(\"Exporting training dataset to GCS\", train_export_path)\n",
    "    export_table_to_gcs(dataset_ref, \n",
    "                        TRAIN_TABLE, \n",
    "                        train_export_path)\n",
    "\n",
    "    print(\"Exporting validation dataset to GCS\", valid_export_path)\n",
    "    export_table_to_gcs(dataset_ref, \n",
    "                        VALID_TABLE, \n",
    "                        valid_export_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create the Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/components/bq2parquet/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/components/bq2parquet/Dockerfile\n",
    "\n",
    "FROM google/cloud-sdk:latest\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install --yes python3-pip\n",
    "\n",
    "COPY . /code\n",
    "WORKDIR /code\n",
    "\n",
    "RUN pip3 install google-cloud-bigquery \n",
    "\n",
    "ENTRYPOINT [\"python3\", \"./main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build and Push to GCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "IMAGE_NAME='bq2parquet'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 4.3 KiB before compression.\n",
      "Uploading tarball of [./pipeline/components/bq2parquet] to [gs://kubeflow-1-0-2_cloudbuild/source/1620468128.924015-e456456c29014da09c76da06ad40ba21.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-1-0-2/locations/global/builds/53299473-5e40-4724-93da-445a1ab9ecb0].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/53299473-5e40-4724-93da-445a1ab9ecb0?project=9118975290].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"53299473-5e40-4724-93da-445a1ab9ecb0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-1-0-2_cloudbuild/source/1620468128.924015-e456456c29014da09c76da06ad40ba21.tgz#1620468129237953\n",
      "Copying gs://kubeflow-1-0-2_cloudbuild/source/1620468128.924015-e456456c29014da09c76da06ad40ba21.tgz#1620468129237953...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   7.68kB\n",
      "Step 1/6 : FROM google/cloud-sdk:latest\n",
      "latest: Pulling from google/cloud-sdk\n",
      "bd8f6a7501cc: Pulling fs layer\n",
      "d6d5ace248d7: Pulling fs layer\n",
      "937080bc07d6: Pulling fs layer\n",
      "60e186286c1b: Pulling fs layer\n",
      "fb4757a9d09f: Pulling fs layer\n",
      "6376d0e9dc08: Pulling fs layer\n",
      "4a4dc2991beb: Pulling fs layer\n",
      "60e186286c1b: Waiting\n",
      "fb4757a9d09f: Waiting\n",
      "6376d0e9dc08: Waiting\n",
      "4a4dc2991beb: Waiting\n",
      "d6d5ace248d7: Verifying Checksum\n",
      "d6d5ace248d7: Download complete\n",
      "bd8f6a7501cc: Verifying Checksum\n",
      "bd8f6a7501cc: Download complete\n",
      "fb4757a9d09f: Verifying Checksum\n",
      "fb4757a9d09f: Download complete\n",
      "60e186286c1b: Verifying Checksum\n",
      "60e186286c1b: Download complete\n",
      "6376d0e9dc08: Verifying Checksum\n",
      "6376d0e9dc08: Download complete\n",
      "4a4dc2991beb: Verifying Checksum\n",
      "4a4dc2991beb: Download complete\n",
      "bd8f6a7501cc: Pull complete\n",
      "d6d5ace248d7: Pull complete\n",
      "937080bc07d6: Verifying Checksum\n",
      "937080bc07d6: Download complete\n",
      "937080bc07d6: Pull complete\n",
      "60e186286c1b: Pull complete\n",
      "fb4757a9d09f: Pull complete\n",
      "6376d0e9dc08: Pull complete\n",
      "4a4dc2991beb: Pull complete\n",
      "Digest: sha256:7f58cc97f622b1e733fb1530d93075b3cf77d5819316dc51ed160ea8755a6a86\n",
      "Status: Downloaded newer image for google/cloud-sdk:latest\n",
      " ---> b20772ebbf0d\n",
      "Step 2/6 : RUN apt-get update &&     apt-get install --yes python3-pip\n",
      " ---> Running in 4c2be34d1728\n",
      "Hit:1 http://deb.debian.org/debian buster InRelease\n",
      "Get:2 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]\n",
      "Get:3 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]\n",
      "Get:4 https://packages.cloud.google.com/apt cloud-sdk-buster InRelease [6774 B]\n",
      "Fetched 124 kB in 1s (175 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "python3-pip is already the newest version (18.1-5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "Removing intermediate container 4c2be34d1728\n",
      " ---> 0eb643a4f4b5\n",
      "Step 3/6 : COPY . /code\n",
      " ---> 269dd0cf63d1\n",
      "Step 4/6 : WORKDIR /code\n",
      " ---> Running in 95613646eccc\n",
      "Removing intermediate container 95613646eccc\n",
      " ---> 918303adeba4\n",
      "Step 5/6 : RUN pip3 install google-cloud-bigquery\n",
      " ---> Running in 60e6283c6757\n",
      "\u001b[91m/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[0mCollecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-2.16.0-py2.py3-none-any.whl (221 kB)\n",
      "Collecting google-api-core[grpc]<2.0.0dev,>=1.23.0\n",
      "  Downloading google_api_core-1.26.3-py2.py3-none-any.whl (93 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-1.2.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting proto-plus>=1.10.0\n",
      "  Downloading proto_plus-1.18.1-py3-none-any.whl (42 kB)\n",
      "Collecting google-auth<2.0dev,>=1.21.1\n",
      "  Downloading google_auth-1.30.0-py2.py3-none-any.whl (146 kB)\n",
      "Collecting six>=1.13.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/lib/python3/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery) (40.8.0)\n",
      "Collecting grpcio<2.0dev,>=1.29.0\n",
      "  Downloading grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl (38 kB)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (2.20)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, googleapis-common-protos, google-auth, grpcio, google-crc32c, google-api-core, proto-plus, google-resumable-media, google-cloud-core, google-cloud-bigquery\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "Successfully installed cachetools-4.2.2 certifi-2020.12.5 chardet-4.0.0 google-api-core-1.26.3 google-auth-1.30.0 google-cloud-bigquery-2.16.0 google-cloud-core-1.6.0 google-crc32c-1.1.2 google-resumable-media-1.2.0 googleapis-common-protos-1.53.0 grpcio-1.37.1 idna-2.10 packaging-20.9 proto-plus-1.18.1 protobuf-3.16.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-2.4.7 pytz-2021.1 requests-2.25.1 rsa-4.7.2 six-1.16.0 urllib3-1.26.4\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 60e6283c6757\n",
      " ---> cbf4f0fe1556\n",
      "Step 6/6 : ENTRYPOINT [\"python3\", \"./main.py\"]\n",
      " ---> Running in 638028d0ac80\n",
      "Removing intermediate container 638028d0ac80\n",
      " ---> 46eae56b13b9\n",
      "Successfully built 46eae56b13b9\n",
      "Successfully tagged gcr.io/kubeflow-1-0-2/bq2parquet:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-1-0-2/bq2parquet:latest\n",
      "The push refers to repository [gcr.io/kubeflow-1-0-2/bq2parquet]\n",
      "9b0f69ddaee4: Preparing\n",
      "55bd83a7ff33: Preparing\n",
      "66134010dfbc: Preparing\n",
      "55fd8e809e8a: Preparing\n",
      "8c37397a33c8: Preparing\n",
      "c57169c94a37: Preparing\n",
      "14e8367e28f0: Preparing\n",
      "1afaa8aa08a0: Preparing\n",
      "31b04620d188: Preparing\n",
      "e2c6ff462357: Preparing\n",
      "c57169c94a37: Waiting\n",
      "14e8367e28f0: Waiting\n",
      "1afaa8aa08a0: Waiting\n",
      "31b04620d188: Waiting\n",
      "e2c6ff462357: Waiting\n",
      "8c37397a33c8: Layer already exists\n",
      "55fd8e809e8a: Layer already exists\n",
      "c57169c94a37: Layer already exists\n",
      "14e8367e28f0: Layer already exists\n",
      "1afaa8aa08a0: Layer already exists\n",
      "31b04620d188: Layer already exists\n",
      "e2c6ff462357: Layer already exists\n",
      "55bd83a7ff33: Pushed\n",
      "66134010dfbc: Pushed\n",
      "9b0f69ddaee4: Pushed\n",
      "latest: digest: sha256:9d34bfbd61a2d877d7ff37de5f3a30b5b85b22bbb8d2e4601771b307244117e4 size: 2424\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                      STATUS\n",
      "53299473-5e40-4724-93da-445a1ab9ecb0  2021-05-08T10:02:09+00:00  1M34S     gs://kubeflow-1-0-2_cloudbuild/source/1620468128.924015-e456456c29014da09c76da06ad40ba21.tgz  gcr.io/kubeflow-1-0-2/bq2parquet (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI ./pipeline/components/bq2parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure to change to project in the GCR.IO path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/components/bq2parquet/component.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/components/bq2parquet/component.yaml\n",
    "\n",
    "name: bq2parquet\n",
    "description: |\n",
    "    This component creates the training and\n",
    "    validation datasets as BiqQuery tables and export\n",
    "    them to parquet files in GCS\n",
    "\n",
    "inputs:\n",
    "    - {name: Export Path , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/kubeflow-1-0-2/bq2parquet\n",
    "        args: [\n",
    "          \"--export_path\", {inputValue: Export Path},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a KFP\n",
    "Builds a kubeflow pipeline that queries BigQuery, creates train/validation tables, exports those tables out to parquet files in GCS, launches an AI Platform training job, then deploys the model to AI Platform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os \n",
    "\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "\n",
    "\n",
    "PIPELINE_TAR = 'creditcard.tar.gz'  # to be written/created at the time of compiling the kfp pipeline\n",
    "BQ2PARQUET_YAML = './pipeline/components/bq2parquet/component.yaml'\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(local_search_paths=None, \n",
    "                                                url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "\n",
    "@dsl.pipeline(name='Credit_card_fraud',\n",
    "              description='Train and deploy ml model to predict credit card fraud')\n",
    "def pipeline(project_id='',\n",
    "            job_dir=''):\n",
    "    # pre-requisite: container image of model training script should be already there in GCR\n",
    "    TRAINER_IMAGE_URI = f'gcr.io/{project_id}/xgboost_fraud_trainer:latest'\n",
    "    export_path = f'gs://{project_id}/credit_card_fraud/data'\n",
    "    model_output_dir = f'gs://{project_id}/credit_card_fraud/models'\n",
    "    \n",
    "    bq2parquet_op = comp.load_component_from_file(BQ2PARQUET_YAML)\n",
    "    bq2parquet = bq2parquet_op(export_path=export_path)\n",
    "    \n",
    "    train_data_path = os.path.join(export_path, \"train.parquet\")\n",
    "    eval_data_path = os.path.join(export_path, \"eval.parquet\")\n",
    "    \n",
    "    train_args = [\n",
    "                '--training_dataset_path', train_data_path,\n",
    "                '--validation_dataset_path', eval_data_path,\n",
    "                '--max_depth', '20',\n",
    "                '--nohptune',\n",
    "                ]\n",
    "    \n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "                                    project_id=project_id,\n",
    "                                    job_dir=job_dir,\n",
    "                                    region='us-central1',\n",
    "                                    master_image_uri=TRAINER_IMAGE_URI,\n",
    "                                    args=train_args).set_display_name(\"CAIP Training Job - XGBoost\")\n",
    "    \n",
    "    deploy_model = mlengine_deploy_op(\n",
    "                                    model_uri=train_model.outputs['job_dir'],\n",
    "                                    project_id=project_id,\n",
    "                                    model_id='cc_fraud_classifier',\n",
    "                                    version_id='xgb',\n",
    "                                    runtime_version='2.3',\n",
    "                                    python_version='3.7',\n",
    "                                    replace_existing_version='True').set_display_name(\"Deployed XGBoost Model to CAIP Predictions\")\n",
    "    \n",
    "\n",
    "    train_model.after(bq2parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get host url from kfp settings\n",
    "# client = kfp.Client(host='https://402de67bf16fcbda-dot-us-central1.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://402de67bf16fcbda-dot-us-central1.pipelines.googleusercontent.com/#/experiments/details/907ba88a-dc37-41e9-8eec-3e52f4268017\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change to your managed kubeflow endpoint\r\n",
    "HOST = \"https://420de6789bf19486fcbda-dot-us-central1.pipelines.googleusercontent.com\"\r\n",
    "client = kfp.Client(host=HOST)\r\n",
    "exp = client.create_experiment(name='credit_card_fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "compiler.Compiler().compile(pipeline, \n",
    "                            PIPELINE_TAR)  # to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://402de67bf16fcbda-dot-us-central1.pipelines.googleusercontent.com/#/runs/details/e6026c38-5cec-4e36-acdf-a50f419e782b\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET = PROJECT_ID\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(f'gs://{BUCKET}/credit_card_fraud/models', JOB_NAME)\n",
    "\n",
    "run = client.run_pipeline(\n",
    "                        experiment_id=exp.id, \n",
    "                        job_name='credit_card_fraud', \n",
    "                        pipeline_package_path='creditcard.tar.gz', \n",
    "                        params={\n",
    "                                'project_id': PROJECT_ID,\n",
    "                                'job_dir': JOB_DIR\n",
    "                                },\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "interpreter": {
   "hash": "9966457a2b43f460ad65e0db6048bd63dc35f6eed439dc25edac406a245d47c9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('autoencoder': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}