{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The google.cloud.bigquery extension is already loaded. To reload it, use:\n",
      "  %reload_ext google.cloud.bigquery\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a master table in BigQuery that joins the transactions & identity tables\n",
    "Need to have a BigQuery dataset called credit_card_fraud and the two tables called train_identity and train_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 4/4 [00:00<00:00, 1075.05query/s]                        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE credit_card_fraud.data AS\n",
    "\n",
    "SELECT * EXCEPT(TransactionID) FROM credit_card_fraud.train_identity AS a\n",
    "INNER JOIN credit_card_fraud.train_transaction AS b\n",
    "ON a.TransactionID = b.TransactionID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create BigQuery tables for training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 3/3 [00:00<00:00, 279.75query/s]                         \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE credit_card_fraud.train AS\n",
    "\n",
    "WITH features_table AS (\n",
    "SELECT IFNULL(TransactionDT, 0) AS TransactionDT, IFNULL(TransactionAmt, 0.0) AS TransactionAmt, IFNULL(card1,0) AS card1, IFNULL(card2,0.0) AS card2, IFNULL(card3,0.0) AS card3, IFNULL(C1,0.0) AS C1, IFNULL(C2,0.0) AS C2, IFNULL(C11,0.0) AS C11, IFNULL(C12,0.0) AS C12, IFNULL(C13,0.0) AS C13, IFNULL(C14,0.0) AS C14, IFNULL(D8,0.0) AS D8, IFNULL(V45,0.0) AS V45, IFNULL(V87,0.0) AS V87, IFNULL(V258,0.0) AS V258, IFNULL(card6, \"Unknown\") AS card6, IFNULL(ProductCD, \"Unknown\") AS ProductCD, IFNULL(P_emaildomain, \"Unknown\") AS emaildomain,isFraud \n",
    "FROM `kubeflow-1-0-2.credit_card_fraud.data`\n",
    "WHERE isFraud IS NOT NULL)\n",
    "\n",
    "SELECT * FROM features_table AS data\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(data))), 10) < 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 3/3 [00:00<00:00, 1691.02query/s]                        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE credit_card_fraud.validation AS\n",
    "\n",
    "WITH features_table AS (\n",
    "SELECT IFNULL(TransactionDT, 0) AS TransactionDT, IFNULL(TransactionAmt, 0.0) AS TransactionAmt, IFNULL(card1,0) AS card1, IFNULL(card2,0.0) AS card2, IFNULL(card3,0.0) AS card3, IFNULL(C1,0.0) AS C1, IFNULL(C2,0.0) AS C2, IFNULL(C11,0.0) AS C11, IFNULL(C12,0.0) AS C12, IFNULL(C13,0.0) AS C13, IFNULL(C14,0.0) AS C14, IFNULL(D8,0.0) AS D8, IFNULL(V45,0.0) AS V45, IFNULL(V87,0.0) AS V87, IFNULL(V258,0.0) AS V258, IFNULL(card6, \"Unknown\") AS card6, IFNULL(ProductCD, \"Unknown\") AS ProductCD, IFNULL(P_emaildomain, \"Unknown\") AS emaildomain,isFraud \n",
    "FROM `kubeflow-1-0-2.credit_card_fraud.data`\n",
    "WHERE isFraud IS NOT NULL)\n",
    "\n",
    "SELECT * FROM features_table AS data\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(data))), 10) >= 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the train and validation data tables in BigQuery to Parquet files in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: kubeflow-1-0-2\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0] \n",
    "BUCKET = PROJECT_ID\n",
    "print(f\"Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be saved to gs://bucket/credit_card_fraud/data/train.parquet and gs://bucket/credit_card_fraud/data/eval.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f'gs://{BUCKET}/credit_card_fraud/data/' \n",
    "TRAIN_DATA_PATH = DATA_PATH + 'train.parquet'\n",
    "EVAL_DATA_PATH = DATA_PATH + 'eval.parquet'\n",
    "ARTIFACT_STORE = f'gs://{PROJECT_ID}-kubeflowpipelines-default'\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r4f6e4f4c2aa3a916_00000178fddfb5fa_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract --destination_format PARQUET $PROJECT_ID:credit_card_fraud.train $TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_rfbb06ea847d6e22_00000178fddfca14_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract --destination_format PARQUET $PROJECT_ID:credit_card_fraud.validation $EVAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to make sure we have the correct versions of everything installed (based on local modelling done beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install --upgrade setuptools\n",
    "# pip install --upgrade xgboost==1.2.1\n",
    "# pip install --upgrade scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Version: 1.2.1\n",
      "Scikit-learn Version: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "# Restart your kernel then run this cell. These should now be 1.2.1 and 0.23.2\n",
    "import xgboost\n",
    "import sklearn\n",
    "print(f\"XGBoost Version: {xgboost.__version__}\")\n",
    "print(f\"Scikit-learn Version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop the train.py file. We will do one hot encoding and scaling in this file to make sure that it happens at prediction time as well. Doing the one hot encoding **BEFORE** creating train.parquet and eval.parquet is bad practice because then we have to do those transformations on the client side at prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./trainer’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/train.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC  # SMOTE but ALSO including categorical features to oversample min class\n",
    "\n",
    "def train_evaluate(job_dir, \n",
    "                   training_dataset_path, \n",
    "                   validation_dataset_path, \n",
    "                   max_depth,\n",
    "                   hptune):\n",
    "    \n",
    "    \n",
    "    # Get training and validation datasets (from GCS buckets)\n",
    "    df_train = pd.read_parquet(training_dataset_path)\n",
    "    df_validation = pd.read_parquet(validation_dataset_path)\n",
    "    \n",
    "    numeric_feature_indexes = list(range(0,15))\n",
    "    categorical_feature_indexes = list(range(15,18))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "                                    transformers=[\n",
    "                                                ('num', StandardScaler(), numeric_feature_indexes),\n",
    "                                                ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "                                    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('classifier', XGBClassifier(max_depth=max_depth))\n",
    "                        ])\n",
    "    \n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "    \n",
    "    # X:y splits, for training and validation\n",
    "    y_train = df_train['isFraud'] # target/label column\n",
    "    X_train = df_train.drop('isFraud', \n",
    "                            axis=1, # columns\n",
    "                            inplace=False) # The original dataframe df remains the same\n",
    "    \n",
    "    y_validation = df_validation['isFraud'] # target/label column\n",
    "    X_validation = df_validation.drop('isFraud', \n",
    "                                      axis=1, # columns\n",
    "                                      inplace=False) # The original dataframe df remains the same\n",
    "    \n",
    "    \n",
    "    X_train, y_train = SMOTENC(categorical_features=categorical_feature_indexes).fit_resample(X_train, \n",
    "                                                                                              np.array(y_train))\n",
    "    \n",
    "    print(\"SMOTE complete. Begin fitting model\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        y_pred = pipeline.predict(X_validation)\n",
    "        predictions = pipeline.predict_proba(X_validation)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_validation,\n",
    "                                predictions)\n",
    "        print('ROC AUC Score: ', roc_auc)\n",
    "        \n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(hyperparameter_metric_tag='roc_auc',  # metric to be optimized\n",
    "                                                metric_value=roc_auc)\n",
    "        \n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Version: 1.2.1\n",
      "Scikit-learn Version: 0.23.2\n",
      "Pandas Version: 1.2.4\n",
      "imbalanced-learn Version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "# To see what versions of libraries we need to include in Dockerfile\n",
    "import pandas\n",
    "import sklearn\n",
    "import xgboost\n",
    "import imblearn\n",
    "\n",
    "print(f\"XGBoost Version: {xgboost.__version__}\")\n",
    "print(f\"Scikit-learn Version: {sklearn.__version__}\")\n",
    "print(f\"Pandas Version: {pandas.__version__}\")\n",
    "print(f\"imbalanced-learn Version: {imblearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package script into Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune imbalanced-learn scikit-learn==0.23.2 pandas==1.2.4 xgboost==1.2.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and push to GCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next cell will build and push Image to: gcr.io/kubeflow-1-0-2/xgboost_fraud_trainer:latest\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='xgboost_fraud_trainer'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "TRAINING_APP_FOLDER='trainer'\n",
    "print(f\"The next cell will build and push Image to: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 4.1 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://kubeflow-1-0-2_cloudbuild/source/1619167029.703525-7251b1d08aef4920958b868c7cc528fc.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-1-0-2/locations/global/builds/71f8ecbf-9ea2-4ce4-8e14-9481e343d196].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/71f8ecbf-9ea2-4ce4-8e14-9481e343d196?project=9118975290].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"71f8ecbf-9ea2-4ce4-8e14-9481e343d196\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-1-0-2_cloudbuild/source/1619167029.703525-7251b1d08aef4920958b868c7cc528fc.tgz#1619167030047338\n",
      "Copying gs://kubeflow-1-0-2_cloudbuild/source/1619167029.703525-7251b1d08aef4920958b868c7cc528fc.tgz#1619167030047338...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   7.68kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "6e0aa5e7af40: Pulling fs layer\n",
      "d47239a868b3: Pulling fs layer\n",
      "49cbb10cca85: Pulling fs layer\n",
      "111e3a13fbd4: Pulling fs layer\n",
      "b35c22f6a9bc: Pulling fs layer\n",
      "8206ae2b86e3: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "a9b2fba996c8: Pulling fs layer\n",
      "57c8f95170d4: Pulling fs layer\n",
      "217c82b1f15a: Pulling fs layer\n",
      "973cc805e91c: Pulling fs layer\n",
      "c425ab4c2c48: Pulling fs layer\n",
      "aed13e0eac71: Pulling fs layer\n",
      "8af440fcd8dd: Pulling fs layer\n",
      "dd95c6ad22f2: Pulling fs layer\n",
      "4e6c4135d19b: Pulling fs layer\n",
      "3d9b09ca1e5d: Pulling fs layer\n",
      "2a6cb8fdb8cf: Pulling fs layer\n",
      "9dd554f1f284: Pulling fs layer\n",
      "b35c22f6a9bc: Waiting\n",
      "8206ae2b86e3: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "a9b2fba996c8: Waiting\n",
      "57c8f95170d4: Waiting\n",
      "217c82b1f15a: Waiting\n",
      "973cc805e91c: Waiting\n",
      "c425ab4c2c48: Waiting\n",
      "aed13e0eac71: Waiting\n",
      "8af440fcd8dd: Waiting\n",
      "dd95c6ad22f2: Waiting\n",
      "4e6c4135d19b: Waiting\n",
      "3d9b09ca1e5d: Waiting\n",
      "2a6cb8fdb8cf: Waiting\n",
      "9dd554f1f284: Waiting\n",
      "111e3a13fbd4: Waiting\n",
      "d47239a868b3: Verifying Checksum\n",
      "d47239a868b3: Download complete\n",
      "49cbb10cca85: Verifying Checksum\n",
      "49cbb10cca85: Download complete\n",
      "111e3a13fbd4: Verifying Checksum\n",
      "111e3a13fbd4: Download complete\n",
      "6e0aa5e7af40: Verifying Checksum\n",
      "6e0aa5e7af40: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "a9b2fba996c8: Verifying Checksum\n",
      "a9b2fba996c8: Download complete\n",
      "8206ae2b86e3: Verifying Checksum\n",
      "8206ae2b86e3: Download complete\n",
      "217c82b1f15a: Verifying Checksum\n",
      "217c82b1f15a: Download complete\n",
      "973cc805e91c: Verifying Checksum\n",
      "973cc805e91c: Download complete\n",
      "57c8f95170d4: Verifying Checksum\n",
      "57c8f95170d4: Download complete\n",
      "c425ab4c2c48: Verifying Checksum\n",
      "c425ab4c2c48: Download complete\n",
      "aed13e0eac71: Verifying Checksum\n",
      "aed13e0eac71: Download complete\n",
      "8af440fcd8dd: Verifying Checksum\n",
      "8af440fcd8dd: Download complete\n",
      "4e6c4135d19b: Verifying Checksum\n",
      "4e6c4135d19b: Download complete\n",
      "dd95c6ad22f2: Verifying Checksum\n",
      "dd95c6ad22f2: Download complete\n",
      "3d9b09ca1e5d: Verifying Checksum\n",
      "3d9b09ca1e5d: Download complete\n",
      "9dd554f1f284: Verifying Checksum\n",
      "9dd554f1f284: Download complete\n",
      "b35c22f6a9bc: Verifying Checksum\n",
      "b35c22f6a9bc: Download complete\n",
      "6e0aa5e7af40: Pull complete\n",
      "d47239a868b3: Pull complete\n",
      "49cbb10cca85: Pull complete\n",
      "111e3a13fbd4: Pull complete\n",
      "2a6cb8fdb8cf: Verifying Checksum\n",
      "2a6cb8fdb8cf: Download complete\n",
      "b35c22f6a9bc: Pull complete\n",
      "8206ae2b86e3: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "a9b2fba996c8: Pull complete\n",
      "57c8f95170d4: Pull complete\n",
      "217c82b1f15a: Pull complete\n",
      "973cc805e91c: Pull complete\n",
      "c425ab4c2c48: Pull complete\n",
      "aed13e0eac71: Pull complete\n",
      "8af440fcd8dd: Pull complete\n",
      "dd95c6ad22f2: Pull complete\n",
      "4e6c4135d19b: Pull complete\n",
      "3d9b09ca1e5d: Pull complete\n",
      "2a6cb8fdb8cf: Pull complete\n",
      "9dd554f1f284: Pull complete\n",
      "Digest: sha256:8d72ecc97cd91b32cfc713ee0613d2cb73aff82c0f53abdd236ba24ec7b7e76c\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 257fdd9e048b\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune imbalanced-learn scikit-learn==0.23.2 pandas==1.2.4 xgboost==1.2.1\n",
      " ---> Running in aa47959768e6\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
      "Collecting scikit-learn==0.23.2\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: pandas==1.2.4 in /opt/conda/lib/python3.7/site-packages (1.2.4)\n",
      "Collecting xgboost==1.2.1\n",
      "  Downloading xgboost-1.2.1-py3-none-manylinux2010_x86_64.whl (148.9 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.4) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.4) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.4) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.2.4) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune, fire, termcolor\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=925f4d871d56e53f6e693e4e12ae033d9825fa6aafe31f6f69c511367497d6b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=0b62b50af8ce77129cd1d40a87f851d08b0bc37071d1b6f0d2f97d568df9709e\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=2e663fc05ad0a99595930fba4ddea9a79ca8444f8cf3ee70c88bf2be29767668\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built cloudml-hypertune fire termcolor\n",
      "Installing collected packages: termcolor, scikit-learn, xgboost, imbalanced-learn, fire, cloudml-hypertune\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 imbalanced-learn-0.7.0 scikit-learn-0.23.2 termcolor-1.1.0 xgboost-1.2.1\n",
      "Removing intermediate container aa47959768e6\n",
      " ---> 8defbc604d6f\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 95a766d4d6a2\n",
      "Removing intermediate container 95a766d4d6a2\n",
      " ---> bdd0700e8f1d\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 22f4e83acc26\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 9e6db0a6703e\n",
      "Removing intermediate container 9e6db0a6703e\n",
      " ---> de634eda8e60\n",
      "Successfully built de634eda8e60\n",
      "Successfully tagged gcr.io/kubeflow-1-0-2/xgboost_fraud_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-1-0-2/xgboost_fraud_trainer:latest\n",
      "The push refers to repository [gcr.io/kubeflow-1-0-2/xgboost_fraud_trainer]\n",
      "e51e43703088: Preparing\n",
      "4ee04b30d39e: Preparing\n",
      "3c88c2556480: Preparing\n",
      "06d47dc1550f: Preparing\n",
      "6d0460e7b91d: Preparing\n",
      "37a8adb4fafb: Preparing\n",
      "25eacd11cb14: Preparing\n",
      "8bc9ffacd92c: Preparing\n",
      "640b32ec2e81: Preparing\n",
      "a803e99a9e4f: Preparing\n",
      "6fd6ff343521: Preparing\n",
      "889232675b1d: Preparing\n",
      "037b4e8512b6: Preparing\n",
      "15d726558b56: Preparing\n",
      "1f2e24bbaee1: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "31152d177e09: Preparing\n",
      "3cec1b907843: Preparing\n",
      "dcc1b8729dd0: Preparing\n",
      "6f15325cc380: Preparing\n",
      "1e77dd81f9fa: Preparing\n",
      "030309cad0ba: Preparing\n",
      "37a8adb4fafb: Waiting\n",
      "25eacd11cb14: Waiting\n",
      "8bc9ffacd92c: Waiting\n",
      "640b32ec2e81: Waiting\n",
      "a803e99a9e4f: Waiting\n",
      "6fd6ff343521: Waiting\n",
      "889232675b1d: Waiting\n",
      "037b4e8512b6: Waiting\n",
      "15d726558b56: Waiting\n",
      "1f2e24bbaee1: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "31152d177e09: Waiting\n",
      "3cec1b907843: Waiting\n",
      "dcc1b8729dd0: Waiting\n",
      "6f15325cc380: Waiting\n",
      "1e77dd81f9fa: Waiting\n",
      "030309cad0ba: Waiting\n",
      "6d0460e7b91d: Layer already exists\n",
      "06d47dc1550f: Layer already exists\n",
      "37a8adb4fafb: Layer already exists\n",
      "25eacd11cb14: Layer already exists\n",
      "8bc9ffacd92c: Layer already exists\n",
      "640b32ec2e81: Layer already exists\n",
      "a803e99a9e4f: Layer already exists\n",
      "6fd6ff343521: Layer already exists\n",
      "889232675b1d: Layer already exists\n",
      "037b4e8512b6: Layer already exists\n",
      "1f2e24bbaee1: Layer already exists\n",
      "15d726558b56: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "31152d177e09: Layer already exists\n",
      "dcc1b8729dd0: Layer already exists\n",
      "3cec1b907843: Layer already exists\n",
      "1e77dd81f9fa: Layer already exists\n",
      "6f15325cc380: Layer already exists\n",
      "030309cad0ba: Layer already exists\n",
      "e51e43703088: Pushed\n",
      "4ee04b30d39e: Pushed\n",
      "3c88c2556480: Pushed\n",
      "latest: digest: sha256:d4ebf9a26818c92deebe7135ea2257fc3f58a2910c1f53310895e439a6e5ba16 size: 4915\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                 STATUS\n",
      "71f8ecbf-9ea2-4ce4-8e14-9481e343d196  2021-04-23T08:37:10+00:00  3M4S      gs://kubeflow-1-0-2_cloudbuild/source/1619167029.703525-7251b1d08aef4920958b868c7cc528fc.tgz  gcr.io/kubeflow-1-0-2/xgboost_fraud_trainer (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create hyperparam tuning yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 2\n",
    "    hyperparameterMetricTag: roc_auc\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 18\n",
    "      maxValue: 23\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gs://kubeflow-1-0-2/credit_card_fraud/data/train.parquet',\n",
       " 'gs://kubeflow-1-0-2/credit_card_fraud/data/eval.parquet')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_PATH, EVAL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the Hyperparameter Tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20210423_084142] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20210423_084142\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20210423_084142\n",
      "jobId: JOB_20210423_084142\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(f'gs://{BUCKET}/credit_card_fraud/models', JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=us-central1 \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config trainer/hptuning_config.yaml \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAIN_DATA_PATH \\\n",
    "--validation_dataset_path=$EVAL_DATA_PATH \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch a training job with the best max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20210423_094843] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20210423_094843\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20210423_094843\n",
      "jobId: JOB_20210423_094843\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(f'gs://{BUCKET}/credit_card_fraud/models', JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=us-central1 \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAIN_DATA_PATH \\\n",
    "--validation_dataset_path=$EVAL_DATA_PATH \\\n",
    "--max_depth=22 \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model to make sure it works :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Created ai platform model [projects/kubeflow-1-0-2/models/cc_fraud_classifier].\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create cc_fraud_classifier \\\n",
    "--region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://kubeflow-1-0-2/credit_card_fraud/models/JOB_20210423_094843'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "ERROR: (gcloud.alpha.ai-platform.versions.create) INVALID_ARGUMENT: Machine type is not available on this endpoint.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'gcloud alpha ai-platform versions create v1 \\\\\\n--model=cc_fraud_classifier \\\\\\n--origin=gs://kubeflow-1-0-2/credit_card_fraud/models/JOB_20210423_094843/ \\\\\\n--framework=scikit-learn \\\\\\n--python-version=3.7 \\\\\\n--runtime-version=2.3 \\\\\\n--region=us-central1 \\\\\\n--machine-type=mls1-c1-m2 \\\\\\n--package-uris=gs://kyles-public-bucket/packages/imbalanced-learn-0.8.0.tar.gz\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c2ef12b0c256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gcloud alpha ai-platform versions create v1 \\\\\\n--model=cc_fraud_classifier \\\\\\n--origin=gs://kubeflow-1-0-2/credit_card_fraud/models/JOB_20210423_094843/ \\\\\\n--framework=scikit-learn \\\\\\n--python-version=3.7 \\\\\\n--runtime-version=2.3 \\\\\\n--region=us-central1 \\\\\\n--machine-type=mls1-c1-m2 \\\\\\n--package-uris=gs://kyles-public-bucket/packages/imbalanced-learn-0.8.0.tar.gz\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2397\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2399\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'gcloud alpha ai-platform versions create v1 \\\\\\n--model=cc_fraud_classifier \\\\\\n--origin=gs://kubeflow-1-0-2/credit_card_fraud/models/JOB_20210423_094843/ \\\\\\n--framework=scikit-learn \\\\\\n--python-version=3.7 \\\\\\n--runtime-version=2.3 \\\\\\n--region=us-central1 \\\\\\n--machine-type=mls1-c1-m2 \\\\\\n--package-uris=gs://kyles-public-bucket/packages/imbalanced-learn-0.8.0.tar.gz\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud alpha ai-platform versions create v1 \\\n",
    "--model=cc_fraud_classifier \\\n",
    "--origin=gs://kubeflow-1-0-2/credit_card_fraud/models/JOB_20210423_094843/ \\\n",
    "--framework=scikit-learn \\\n",
    "--python-version=3.7 \\\n",
    "--runtime-version=2.3 \\\n",
    "--region=us-central1 \\\n",
    "--machine-type=mls1-c1-m2 \\\n",
    "--package-uris=gs://kyles-public-bucket/packages/imbalanced-learn-0.8.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Awesome! Now let's jump over to kfp_credit_card_fraud.ipynb and build a kubeflow pipeline."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "interpreter": {
   "hash": "9966457a2b43f460ad65e0db6048bd63dc35f6eed439dc25edac406a245d47c9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('autoencoder': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}